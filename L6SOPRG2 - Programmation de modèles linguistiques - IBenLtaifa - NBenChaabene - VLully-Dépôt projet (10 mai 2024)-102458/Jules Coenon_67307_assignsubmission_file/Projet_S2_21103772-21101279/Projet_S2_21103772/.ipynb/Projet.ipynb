{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeFvmPx53f6U"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sat May  4 16:21:26 2024\n",
        "\n",
        "@author: julco\n",
        "\"\"\"\n",
        "########## IMPORTATION DE LIBRAIRIES ##########\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "import string\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "import glob\n",
        "import pandas\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "########## CREATION DE FONCTIONS ##########\n",
        "\n",
        "# permet de lire un fichier et renvoie une chaine de caractères\n",
        "def lirefich(chemin):#lire un fichier\n",
        "    with open(chemin, encoding=\"utf-8\") as r:\n",
        "        chaine = r.read()\n",
        "    return chaine\n",
        "\n",
        "def n_gramme(listee,nb=int(input(\"donner in chiffre\"))):\n",
        "    nbr_gram=[]\n",
        "    for elt in listee:\n",
        "        if len(elt) > 1:\n",
        "           nbr_gram.append(elt[:nb])\n",
        "    return nbr_gram\n",
        "\n",
        "# permet de récupérer les tokens d'un texte à partir du format Doc de spacy et renvoie une liste, calcul de la durée d'exécution également\n",
        "def tokeniser_spacy(texte):\n",
        "  debut = time.time()\n",
        "  liste_tokens = []\n",
        "  for token in texte:\n",
        "    liste_tokens.append(token.text)\n",
        "  print(\"Récupération des tokens OK\")\n",
        "  fin = time.time()\n",
        "  durée = fin - debut\n",
        "  print(\"La tokenisation avec spacy dure:\", durée)\n",
        "  return liste_tokens\n",
        "\n",
        "# permet de récupérer les tokens d'un texte en renvoyant une liste\n",
        "def tokeniser_split(texte):\n",
        "    debut = time.time()\n",
        "    texte = texte.split()\n",
        "    fin = time.time()\n",
        "    durée = fin - debut\n",
        "    print(\"La tokenisation avec la fonction split dure:\", durée)\n",
        "    return texte\n",
        "\n",
        "# récupère les lemmes à partir du format Doc de spacy\n",
        "def lemmatiser(texte):\n",
        "    liste_lemmes=[]\n",
        "    for mot in texte:\n",
        "        liste_lemmes.append(mot.lemma_)\n",
        "    print(\"Récupération des lemmes OK\")\n",
        "    return liste_lemmes\n",
        "\n",
        "# récupère les entités à partir du format Doc de spacy\n",
        "def trouver_entites(texte):\n",
        "    liste_entites = []\n",
        "    for w in texte.ents:\n",
        "        liste_entites.append(w.text)\n",
        "    print(\"Récupération des entités nommées OK\")\n",
        "    return liste_entites\n",
        "\n",
        "### utilisation des fonction récupérer_vocab_manu et trouver_vocab\n",
        "# permet de récupérer le vocabulaire à partir d'un Doc en utilisant le Pos-Tagging de spacy et renvoie une liste qui sera lue par la fonction trouver_vocab\n",
        "# et qui séléctionnera les éléments selon leurs Tag\n",
        "def recuperer_vocab_manu(texte):\n",
        "    debut = time.time()\n",
        "    liste_pos_tag = []\n",
        "    for m in texte:\n",
        "        liste_pos_tag.append([m.text, m.pos_, m.is_stop])\n",
        "    vocabulaire = trouver_vocab(liste_pos_tag)\n",
        "    fin = time.time()\n",
        "    durée = fin - debut\n",
        "    print(\"Trouver le vocabulaire de manière manuelle prend: \", durée)\n",
        "    return vocabulaire\n",
        "\n",
        "def trouver_vocab(liste_pos): #genere un vocabulaire en excluant les stop words, punct, nom propre et space\n",
        "    voc=[]\n",
        "    for mini_liste in liste_pos:\n",
        "            if mini_liste[1]!= \"PROPN\":\n",
        "                if mini_liste[1] != \"SPACE\":\n",
        "                    if mini_liste[1] != \"PUNCT\":\n",
        "                        if mini_liste[2]!= True:\n",
        "                           voc.append(mini_liste[0])\n",
        "    return voc\n",
        "\n",
        "\n",
        "def nettoyer_texte(texte):\n",
        "    exclude_chars = string.punctuation + string.digits\n",
        "    translation_table = str.maketrans(\"\", \"\", exclude_chars)\n",
        "    cleaned_text = texte.translate(translation_table)\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "### Utilisation des fonction nettoyer_texte, nettoyer_liste et recuperer vocab_auto\n",
        "# def nettoyer_texte(texte):\n",
        "#     # Supprimer les signes de ponctuation\n",
        "#     texte_sans_ponctuation = re.sub(r'[^\\w\\s]', '', texte)\n",
        "#     # Supprimer les chiffres\n",
        "#     #texte_sans_chiffres = re.sub(r'\\d', '', texte_sans_ponctuation)\n",
        "#     # Supprimer les parenthèses (et le contenu entre elles si nécessaire)\n",
        "#     #texte_nettoye = re.sub(r'\\(.*?\\)', '', texte_sans_chiffres)\n",
        "#     return texte_sans_ponctuation\n",
        "\n",
        "# def nettoyer_liste(liste):\n",
        "#     liste_propre = [nettoyer_texte(mot) for mot in liste if not mot.startswith('http')]\n",
        "#     return [element for element in liste_propre if element]\n",
        "\n",
        "def recuperer_vocab_auto(texte, language):\n",
        "    debut = time.time()\n",
        "    liste = tokeniser_spacy(texte)\n",
        "    stop_words = set(stopwords.words(f\"{language}\"))\n",
        "    liste_filtree = [mot for mot in liste if mot.lower() not in stop_words]\n",
        "    # liste_propre = nettoyer_liste(liste_filtree)\n",
        "    fin = time.time()\n",
        "    durée = fin - debut\n",
        "    print(\"Trouver le vocabulaire automatiquement prend: \", durée)\n",
        "    return liste_filtree\n",
        "\n",
        "# permet d'enregistrer des listes au format JSON\n",
        "def sauvegarder_json(liste, nom_liste, langue):\n",
        "    nom_fichier = f\"{nom_liste}_spacy_{langue}.json\"\n",
        "    with open(nom_fichier, \"w\", encoding=\"utf-8\") as json_file:\n",
        "        json.dump(liste, json_file, ensure_ascii=False, indent=2)\n",
        "    print(f\"Vocabulaire sauvegardé dans {nom_fichier} avec succès !\")\n",
        "\n",
        "\n",
        "########## MAIN ##########\n",
        "\n",
        "liste_modeles_langue = [\"fr_core_news_sm\", \"en_core_web_sm\", \"es_core_news_sm\"]\n",
        "liste_langues_nltk = [\"french\", \"english\", \"spanish\"]\n",
        "\n",
        "#######\n",
        "i = 1 # permet de changer simultanément la langue de la librairie NLTK selon le modèle de langue spacy choisi, les deux listes du dessus sont rangées dans le même ordre de langues\n",
        "#######\n",
        "\n",
        "modele = liste_modeles_langue[i]\n",
        "langue = modele.split(\"_\")[0]# permet de récupérer la langue dans le modèle spacy afin d'automatiser l'enregistrement de la langue dans le fichier JSON\n",
        "print(langue)\n",
        "\n",
        "nlp = spacy.load(modele)# chargement du modèle de langue spacy choisie avec la variable i\n",
        "nlp.max_length = 2500000# textes longs donc il faut augmenter la capcité de la librairie\n",
        "\n",
        "\n",
        "# mettre une boucle ici pour faire les 3 modèles simultanément ????????????? il faudrait aussi ajouter un i += 1 à la fin de la boucle por changer de langue nltk\n",
        "# chemin = \"/content/drive/MyDrive/Colab/fr/test/*\" # chemin pour Google Colab\n",
        "chemin = f\"corpus_multi/corpus_multi/{langue}/*/*\" # chemin pour Spyder\n",
        "\n",
        "liste_textes = []\n",
        "for path in glob.glob(chemin):# parcourt tous les textes du chemin et les ajoute dans une liste pour traiter langue par langue et non pas texte par texte\n",
        "    texte = lirefich(path)\n",
        "    texte_nettoye = nettoyer_texte(texte)\n",
        "    liste_textes.append(texte_nettoye)\n",
        "print(\"Il y a \", len(liste_textes), \"textes\")\n",
        "textes_string = \" \".join(liste_textes)# transforme la liste des textes en string pour utiliser la librairie spacy\n",
        "print(textes_string[:500])\n",
        "\n",
        "doc = nlp(textes_string)# utilisation de la librairie spacy pour et transforme le format string en format doc\n",
        "# liste_tokens = liste_textes_string.split()\n",
        "liste_tokens_split = tokeniser_split(textes_string)# utilisation de la fonction split() pour récupérer les tokens des textes de la langue choisie\n",
        "liste_tokens_spacy = tokeniser_spacy(doc)# utilisation de la fonction Tokeniser de spacy pour récupérer les tokens\n",
        "liste_lemmes = lemmatiser(doc)# utilisation de la fonction Lemmatizer de spacy pour récupérer les lemmes\n",
        "liste_entites = trouver_entites(doc)# utilisation de la fonction Entity Recognizer de spacy\n",
        "liste_vocab_auto = recuperer_vocab_auto(doc, liste_langues_nltk[i])# on récupère le vocabulaire des textes de la langue choisie\n",
        "print(liste_vocab_auto[:500])\n",
        "liste_vocab_manuel = recuperer_vocab_manu(doc)# idem\n",
        "print(liste_vocab_manuel[:500])\n",
        "\n",
        "sauvegarder_json(liste_vocab_manuel, \"liste_vocab_manuel\", langue)# enregistrement au format JSON du vocabulaire\n",
        "sauvegarder_json(liste_vocab_auto, \"liste_vocab_auto_propre\", langue)# idem"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CODE POUR CLUSTERING\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Jul 11 11:16:20 2022\n",
        "\n",
        "@author: antonomaz\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.cluster import AffinityPropagation\n",
        "from sklearn.metrics import DistanceMetric\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import sklearn\n",
        "import json\n",
        "import glob\n",
        "import re\n",
        "from collections import OrderedDict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def sauvegarder_json(dico, nom_liste, langue):\n",
        "    nom_fichier = f\"{nom_liste}_{langue}.json\"\n",
        "    with open(nom_fichier, \"w\", encoding=\"utf-8\") as json_file:\n",
        "        json.dump(dico, json_file, ensure_ascii=False, indent=2)\n",
        "    print(f\"Cluster sauvegardé dans {nom_fichier} avec succès !\")\n",
        "\n",
        "def n_gramme(listee,nb=int(input(\"donner in chiffre\"))):\n",
        "    nbr_gram=[]\n",
        "    for elt in listee:\n",
        "        if len(elt) > 1:\n",
        "            nbr_gram.append(elt[:nb])\n",
        "    return nbr_gram\n",
        "\n",
        "def nomfichier(chemin):\n",
        "    nomfich= chemin.split(\"/\")[-1]\n",
        "    nomfich= nomfich.split(\".\")\n",
        "    nomfich= (\"_\").join([nomfich[0],nomfich[1]])\n",
        "    return nomfich\n",
        "\n",
        "def lire_json(chemin_fichier_json):\n",
        "    with open(chemin_fichier_json, \"r\", encoding=\"utf-8\") as r:\n",
        "        fich_json = json.load(r)\n",
        "    return fich_json\n",
        "\n",
        "\n",
        "chemin_entree = [\"vocab_manuel_français.json\",\"vocab_auto_français.json\",\"vocab_manuel_anglais.json\",\"vocab_auto_anglais.json\"]\n",
        "\n",
        "\n",
        "def cluster(chemin):\n",
        "#for subcorpus in glob.glob(path_copora):\n",
        "#    print(\"SUBCORPUS***\",subcorpus)\n",
        " #   liste_nom_fichier =[]\n",
        "    for path in glob.glob(chemin):\n",
        "    #        print(\"PATH*****\",path)\n",
        "            liste_nom_fichier=[]\n",
        "            nom_fichier = nomfichier(path)\n",
        "    #        print(nom_fichier)\n",
        "            liste= lire_json(path)\n",
        "            liste_n_gramme= n_gramme(liste)\n",
        "            print(\"les n_grammes ont été récupéré\")\n",
        "\n",
        "    #### FREQUENCE ########\n",
        "\n",
        "            dic_mots={}\n",
        "            i=0\n",
        "\n",
        "\n",
        "            for mot in liste_n_gramme:\n",
        "\n",
        "                if mot not in dic_mots:\n",
        "                    dic_mots[mot] = 1\n",
        "                else:\n",
        "                    dic_mots[mot] += 1\n",
        "\n",
        "            i += 1\n",
        "\n",
        "            new_d = OrderedDict(sorted(dic_mots.items(), key=lambda t: t[0]))\n",
        "\n",
        "            freq=len(dic_mots.keys())\n",
        "\n",
        "\n",
        "            Set_00 = set(liste_n_gramme)\n",
        "            Liste_00 = list(Set_00)\n",
        "            dic_output = {}\n",
        "            liste_words=[]\n",
        "            matrice=[]\n",
        "\n",
        "            for l in Liste_00:\n",
        "\n",
        "                if len(l)!=1:\n",
        "                    liste_words.append(l)\n",
        "\n",
        "\n",
        "            try:\n",
        "                words = np.asarray(liste_words)\n",
        "                for w in words:\n",
        "                    liste_vecteur=[]\n",
        "\n",
        "\n",
        "                    for w2 in words:\n",
        "\n",
        "                            V = CountVectorizer(ngram_range=(2,3), analyzer='char')\n",
        "                            X = V.fit_transform([w,w2]).toarray()\n",
        "                            distance_tab1=sklearn.metrics.pairwise.cosine_distances(X)\n",
        "                            liste_vecteur.append(distance_tab1[0][1])\n",
        "\n",
        "                    matrice.append(liste_vecteur)\n",
        "                matrice_def=-1*np.array(matrice)\n",
        "\n",
        "\n",
        "                affprop = AffinityPropagation(affinity=\"precomputed\", damping= 0.6, random_state = None)\n",
        "\n",
        "                affprop.fit(matrice_def)\n",
        "                for cluster_id in np.unique(affprop.labels_):\n",
        "                    exemplar = words[affprop.cluster_centers_indices_[cluster_id]]\n",
        "                    cluster = np.unique(words[np.nonzero(affprop.labels_==cluster_id)])\n",
        "                    cluster_str = \", \".join(cluster)\n",
        "                    cluster_list = cluster_str.split(\", \")\n",
        "\n",
        "                    Id = \"ID \"+str(i)\n",
        "                    for cle, dic in new_d.items():\n",
        "                        if cle == exemplar:\n",
        "                            dic_output[Id] ={}\n",
        "                            dic_output[Id][\"Centroïde\"] = exemplar\n",
        "                            dic_output[Id][\"Freq. centroide\"] = dic\n",
        "                            dic_output[Id][\"Termes\"] = cluster_list\n",
        "\n",
        "                    i=i+1\n",
        "                return dic_output\n",
        "\n",
        "\n",
        "\n",
        "            except :\n",
        "               # print(\"**********Non OK***********\", path)\n",
        "\n",
        "\n",
        "                liste_nom_fichier.append(path)\n",
        "\n",
        "\n",
        "               # continue\n",
        "\n",
        "#ENREGISTRER LES N-GRAMMES EN FORMAT JSON\n",
        "\n",
        "#bi_gramme_vocab_manuel_fr= sauvegarder_json(cluster(chemin_entree[0]), \"bi_gramme_vocab_manuel\", \"fr\")\n",
        "#tri_gramme_vocab_manuel_fr= sauvegarder_json(cluster(chemin_entree[0]),\"tri_gramme_vocab_manuel\",\"fr\")\n",
        "#quatre_gramme= sauvegarder_json(cluster(chemin_entree[2]), \"quatre_gramme_vocab_manuel\", \"ang\")\n",
        "#cinq_gramme= sauvegarder_json(cluster(chemin_entree[2]),\"cinq_gramme_vocab_manuel\",\"ang\")\n",
        "\n",
        "# #REPRESENTATION GRAPHIQUE DES CLUSTERS\n",
        "#  #liste où on stockera les donnees pour le graphique\n",
        "# cluster_len= []\n",
        "# centroid= []\n",
        "\n",
        "# entree_json= \"les_n_grammes_json/cinq_gramme_vocab_manuel_ang.json\"\n",
        "\n",
        "# for path2 in glob.glob(entree_json):\n",
        "#     fichier= lire_json(path2)\n",
        "\n",
        "#     # calculer la longueure des clusters et les ajouter dans liste\n",
        "#     for cluster2 in fichier:\n",
        "#         cluster_length= len(fichier[cluster2][\"Termes\"])\n",
        "#         cluster_len.append(cluster_length)\n",
        "#         centroid.append(fichier[cluster2][\"Centroïde\"])\n",
        "\n",
        "# #realisation des graphiques\n",
        "# plt.figure(figsize=(20,14)) #taille du graphique\n",
        "# for i , (length, centroids) in enumerate(zip(cluster_len,centroid)):\n",
        "#     plt.scatter(i,length,s= length*15, alpha=0.7, label=f\"Cluster{i}\") #personnaliser les cercles du graphqiues\n",
        "#     plt.annotate(centroids,(i,length),textcoords= \"offset points\", xytext=(0,6), ha=\"center\",fontsize=\"8\")# personaliser le titre des cercles\n",
        "\n",
        "# plt.xlabel(\"cluster index\")\n",
        "# plt.ylabel(\"longueur des clusters\")\n",
        "# plt.title(\"taille des clusters pour cinq-grammes ang\")\n",
        "# plt.grid(True)\n",
        "# #plt.savefig(\"graphique cinq-gramme ang\")\n",
        "# #plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "t-16Vtn5tuxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CODE POUR LES REPRESENTATION GRAPHIQUE\n",
        "\n",
        "#REPRESENTATION GRAPHIQUE\n",
        "\n",
        "#token\n",
        "width= 0.1 #specifie la largeur de chaque bar\n",
        "token_langues=[22408,21306,18052] #donnee a representer dans notre graphique\n",
        "langue=[\"fr\",\"es\",\"en\"] #label qui corresponds à chaque donnee ci-dessus\n",
        "x2= np.arange(len(langue)) # permet de representer le graphique correctement en associant les donnees avec le label qui le corresponds\n",
        "fig2,ax2= plt.subplots() #doit etre redefini sinon les graph se superposent\n",
        "\n",
        "graph2= plt.bar(range(len(token_langues)),token_langues) #dessine le graphique\n",
        "plt.bar_label(graph2, padding=3) #permet d'ajouter des label au graphique\n",
        "graph2[0].set_color(\"blue\") #couleur de la premiere bar\n",
        "graph2[1].set_color(\"pink\") #couleur de la deuxième\n",
        "graph2[2].set_color(\"purple\") #couleur de la troisième\n",
        "\n",
        "plt.title(\"nbr de tokens pour chaque langue\")\n",
        "\n",
        "ax2.set_xticks(x2+width, langue) # place correctement le label par rapport à la bar qui le corresponds\n",
        "\n",
        "#plt.savefig(\"nbr tokens langues\")\n",
        "#plt.show()\n",
        "\n",
        "#lemmatisation\n",
        "lemmes_langues=[22408,21306,18052]\n",
        "fig3, ax3= plt.subplots()\n",
        "\n",
        "graph3= plt.bar(range(len(lemmes_langues)),lemmes_langues)\n",
        "plt.bar_label(graph3, padding= 3)\n",
        "graph3[0].set_color(\"red\")\n",
        "graph3[1].set_color(\"green\")\n",
        "graph3[2].set_color(\"orange\")\n",
        "\n",
        "plt.title(\"nbr de lemmes pour chaque langue\")\n",
        "\n",
        "ax3.set_xticks(x2+width, langue)\n",
        "\n",
        "#plt.show()\n",
        "#plt.savefig(\"nbr lemme langues\")\n",
        "\n",
        "#ent\n",
        "ent_langues=[1062,1385,1500]\n",
        "fig4, ax4= plt.subplots()\n",
        "\n",
        "graph4= plt.bar(range(len(ent_langues)),ent_langues)\n",
        "plt.bar_label(graph4, padding=3)\n",
        "graph4[0].set_color(\"yellow\")\n",
        "graph4[1].set_color(\"cyan\")\n",
        "graph4[2].set_color(\"pink\")\n",
        "\n",
        "plt.title(\"nbr de ent pour chaque langues\")\n",
        "\n",
        "ax4.set_xticks(x2+width, langue)\n",
        "#plt.show()\n",
        "#plt.savefig(\"nbr ent langues\")\n",
        "\n",
        "#comparaison de la taille des vocab manuel et automatique\n",
        "width= 0.1\n",
        "vocab_langues=[12484,6895,17362,7033,15742,8744]\n",
        "type_vocab=[\"en_nltk\",\"en_manuel\",\"es_nltk\",\"es_manuel\",\"fr_nltk\",\"fr_manuel\"]\n",
        "x3= np.arange(len(type_vocab))\n",
        "fig5,ax5= plt.subplots() #doit etre redefini sinon les graph se superposent\n",
        "\n",
        "graph5= plt.bar(range(len(vocab_langues)),vocab_langues)\n",
        "plt.bar_label(graph5, padding=3)\n",
        "graph5[0].set_color(\"yellow\")\n",
        "graph5[1].set_color(\"yellow\")\n",
        "graph5[2].set_color(\"blue\")\n",
        "graph5[3].set_color(\"blue\")\n",
        "graph5[4].set_color(\"orange\")\n",
        "graph5[5].set_color(\"orange\")\n",
        "\n",
        "plt.title(\"comparaison des vocabulaires\")\n",
        "\n",
        "ax5.set_xticks(x3+width,type_vocab)\n",
        "\n",
        "#plt.savefig(\"comparer vocab\")\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "6YUMy_uH9TDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MGga-Yk4QuM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRAPHIQUE CLUSTER\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Fri Mar  8 19:20:48 2024\n",
        "\n",
        "@author: julco\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import json\n",
        "\n",
        "def lire_json(chemin):\n",
        "    with open(chemin, \"r\", encoding=\"UTF-8\") as f:\n",
        "        chaine = json.load(f)\n",
        "    return chaine\n",
        "\n",
        "# Initialise des listes vides pour les remplir par la suite afin d'y stocker des informations pour le graphique\n",
        "cluster_lengths = []\n",
        "centroids = []\n",
        "\n",
        "entree = \"\"\n",
        "# Parcourt le dossier pour lire les fichiers JSON\n",
        "for chemin in glob.glob(entree):\n",
        "    nom_fichier = chemin.split(\"\\\\\")[-1]\n",
        "    print(nom_fichier)\n",
        "    fichier = lire_json(chemin)\n",
        "\n",
        "    # Permet de calculer la longueur des clusters et de les ajouter à une liste\n",
        "    for cluster in fichier:\n",
        "        cluster_length = len(fichier[cluster][\"Termes\"])\n",
        "        cluster_lengths.append(cluster_length)\n",
        "        centroids.append(fichier[cluster][\"Centroïde\"])\n",
        "\n",
        "# Création d'un graphique qui représente la longueur des différents clusters sous forme de cercles plus ou moins important\n",
        "plt.figure(figsize=(20, 14))# gère la taille du graphique\n",
        "for i, (length, centroid) in enumerate(zip(cluster_lengths, centroids)):\n",
        "    plt.scatter(i, length, s = length * 10, alpha =  0.7, label = f\"Cluster {i}\")# personnalisation des cercles du graphique\n",
        "    plt.annotate(centroid, (i, length), textcoords = \"offset points\", xytext = (0,6), ha = \"center\", fontsize = \"8\")# permet de personnaliser le titre des cercles\n",
        "\n",
        "# Personnalisation du graphique et enregsitrement de ce-dernier\n",
        "plt.xlabel(\"Cluster Index\")\n",
        "plt.ylabel(\"Longueur des cluster\")\n",
        "plt.title(\"Longueur de cluster selon les centroides\")\n",
        "plt.grid(True)\n",
        "# plt.savefig(\"Graphique_cluster.png\")\n",
        "# plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "2Ycvl6d33YNp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}