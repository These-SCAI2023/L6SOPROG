{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007bc3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0d5f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping des langues aux modèles SpaCy correspondants\n",
    "modeles_langue = {\n",
    "    'en': 'en_core_web_sm',\n",
    "    'fr': 'fr_core_news_sm',\n",
    "    'es': 'es_core_news_sm'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36534e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour lire les fichiers texte\n",
    "def lire_fichier(dossier_corpus):\n",
    "    fichiers = []\n",
    "    # Chemin vers les fichiers texte dans le dossier corpusM\n",
    "    chemin_fichiers_texte = f\"{dossier_corpus}/*/*/*.html\"\n",
    "\n",
    "    # Utiliser glob pour récupérer la liste des chemins des fichiers texte\n",
    "    for chemin_fichier in glob.glob(chemin_fichiers_texte):\n",
    "        # Lire le contenu du fichier\n",
    "        with open(chemin_fichier, 'r', encoding='utf-8') as fichier:\n",
    "            texte = fichier.read()\n",
    "        \n",
    "        # Extraire la langue à partir du nom du fichier\n",
    "        langue = chemin_fichier.split(\"/\")[-3]\n",
    "        fichiers.append((texte, langue))\n",
    "\n",
    "    return fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a885dd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour charger les modèles SpaCy\n",
    "def charger_modele_spacy(langue):\n",
    "    if langue not in modeles_langue:\n",
    "        print(f\"Langue non supportée: {langue}\")\n",
    "        return None\n",
    "    return spacy.load(modeles_langue[langue])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a10591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction qui tokenise les textes\n",
    "def tokenizer_texte(texte, langue= None):\n",
    "    nltk.download('punkt')\n",
    "    # Tokenize le texte en utilisant le tokenizer punkt de nltk\n",
    "    return word_tokenize(texte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54762e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction qui lemmatise le texte\n",
    "def lemmatiser_texte(tokens, langue= None):\n",
    "    nlp = charger_modele_spacy(langue)\n",
    "    if nlp is None:\n",
    "        return []\n",
    "    # Lemmatize chaque token dans la liste\n",
    "    return [token.lemma_ for token in nlp(\" \".join(tokens))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a384d402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction qui effectue la REN\n",
    "def reconnaitre_entites_nommees(texte, langue= None):\n",
    "    nlp = charger_modele_spacy(langue)\n",
    "    if nlp is None:\n",
    "        return []\n",
    "    # Effectuer la REN sur le texte\n",
    "    doc = nlp(texte)\n",
    "    # Récupérer les entités nommées détectées dans le texte\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5475f5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction qui permet de supprimer les mots vides et les noms propres des textes\n",
    "def filtrer_mots(texte, langue= None):\n",
    "\n",
    "    # Charger le modèle SpaCy correspondant à la langue spécifiée\n",
    "    nlp = charger_modele_spacy(langue)\n",
    "    if nlp is None:\n",
    "        return [], []\n",
    "\n",
    "    # Effectuer le traitement du texte avec SpaCy\n",
    "    doc = nlp(texte)\n",
    "\n",
    "    # Exclure les mots vides et conserver les noms propres\n",
    "    tokens_filtres = [token.text for token in doc if not token.is_stop]\n",
    "    noms_propres = [token.text for token in doc if token.pos_ == 'PROPN']\n",
    "\n",
    "    return tokens_filtres, noms_propres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606e570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recup_info(fichiers):\n",
    "    info = {}\n",
    "    for texte, langue in fichiers:\n",
    "        # Tokenisation\n",
    "        tokens = word_tokenize(texte)\n",
    "\n",
    "        # Nombre de tokens par texte pour chaque langue\n",
    "        info.setdefault(langue, {}).setdefault('nb_tokens_par_texte', []).append(len(tokens))\n",
    "\n",
    "        # Nombre de token type (vocabulaire) par texte pour chaque langue\n",
    "        nb_token_type = len(set(tokens))\n",
    "        info.setdefault(langue, {}).setdefault('nb_token_type_par_texte', []).append(nb_token_type)\n",
    "\n",
    "        # Proportion de lemmes par texte pour chaque langue\n",
    "        nlp = charger_modele_spacy(langue)\n",
    "        if nlp is None:\n",
    "            continue\n",
    "        lemmes = [token.lemma_ for token in nlp(\" \".join(tokens))]\n",
    "        proportion_lemmes = len(lemmes) / len(tokens)\n",
    "        info.setdefault(langue, {}).setdefault('proportion_lemmes_par_texte', []).append(proportion_lemmes)\n",
    "\n",
    "        # Nombre de lemmes total par langue\n",
    "        nb_lemmes_total = len(lemmes)\n",
    "        info.setdefault(langue, {}).setdefault('nb_lemmes_total', 0)\n",
    "        info[langue]['nb_lemmes_total'] += nb_lemmes_total\n",
    "\n",
    "        # Proportion de noms propres pour chaque langue\n",
    "        noms_propres = [token.text for token in nlp(\" \".join(tokens)) if token.pos_ == 'PROPN']\n",
    "        proportion_noms_propres = len(noms_propres) / len(tokens)\n",
    "        info.setdefault(langue, {}).setdefault('proportion_noms_propres_par_texte', []).append(proportion_noms_propres)\n",
    "\n",
    "    # Calcul du nombre de tokens global par langue\n",
    "    for langue, values in info.items():\n",
    "        values['nb_tokens_global'] = sum(values['nb_tokens_par_texte'])\n",
    "\n",
    "    # Sauvegarde des informations dans un fichier JSON\n",
    "    sauvegarder_info(info)\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde02c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sauvegarder_info(info):\n",
    "    with open('informationsV1.2C.json', 'w') as f:\n",
    "        json.dump(info, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d02a922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploitation\n",
    "dossier_corpus = \"corpusM\"\n",
    "fichiers = lire_fichier(dossier_corpus)\n",
    "# Prétraitement des fichiers\n",
    "for texte, langue in fichiers:\n",
    "    # Tokenisation et lemmatisation du texte\n",
    "    tokens = tokenizer_texte(texte, langue=langue)\n",
    "    tokens_lemmatises = lemmatiser_texte(tokens, langue=langue)\n",
    "    \n",
    "    # REN du texte\n",
    "    entites_nommees = reconnaitre_entites_nommees(texte, langue=langue)\n",
    "    \n",
    "    # Filtrage des mots vides et des noms propres\n",
    "    tokens_filtres, noms_propres = filtrer_mots(texte, langue=langue)\n",
    "\n",
    "# Exploitation\n",
    "infos = recup_info(fichiers)\n",
    "sauvegarder_info(infos)\n",
    "\n",
    "    #Affichage des informations\n",
    "    # print(\"Langue du fichier :\", langue)\n",
    "    # print(\"Tokens lemmatisés :\", tokens_lemmatises)\n",
    "    # print(\"Entités nommées :\", entites_nommees)\n",
    "    # print(\"Tokens filtrés :\", tokens_filtres)\n",
    "    # print(\"Noms propres :\", noms_propres)\n",
    "    # print(\"------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
