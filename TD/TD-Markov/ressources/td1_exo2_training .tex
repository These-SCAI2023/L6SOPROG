Une fois que nous avons les données prêtes, nous pouvons procéder à l'entraînement.

\subsection{Étape 1 : Création des représentations matricielles}
Pour commencer l'entraînement nous devons déclarer les objets qui nous serviront pour traiter les données et effectuer les calculs nécessaires. Rappelez que une façon d’illustrer une chaîne de Markov peut être au moyen d'un graphe ou d'une matrice.

Créez deux matrices $A0$ et $A1$ avec une dimensions égal a $V$ où $V = $ \textit{len(word2idx)}. La matrice $A0$ servira pour gérer les probabilités de passer d'un état $t$ étant donné l'état $t-1$ issues du corpus \textit{Poe} et la matrice $A1$ pour le corpus \textit{Frost}. Créez aussi deux listes $pi0$ et $pi1$ pour gérer les probabilités d'apparition en début de phrase des mots de chaque corpus.

Toutes les liste doivent être initialisées en 1. Utilisez la fonction ones() de la bibliothèque \textit{Numpy} comme dans l'exemple.

\begin{python}
#Create a matrix of dimension 10 x 10 initialized by ones
np.ones((10, 10))

#Create a list or vector of 10 initialized by ones
np.ones(10)
\end{python}

\subsection{Étape 2 : Extraction de caractéristiques stochastiques}

Ensuite créez la fonction \textit{compute\_counts()}, cette fonction doit recevoir l'objet \textit{text\_as\_int} crée dans l'exercice précédent; la matrice $A$ et la la liste $pi$ qui seront affectées avec les caractéristiques issues de \textit{text\_as\_int}.
\begin{python}
# compute counts for A and pi
def compute_counts(text_as_int, A, pi):
\end{python}

À l’intérieure de la fonction lisez \textit{compute\_counts()} pour extraire et comptabiliser les couples ou bigrammes de mots et les affecter dans la matrice $A$. Identifiez et comptabilisez aussi les mots qui apparaissent en début de phrases et gardez le résultat dans la liste $pi$. Pas besoin de \textit{return} dans cette fonction. 

\textcolor{red}{Vous pouvez vous inspirer du code suivant, attention qu'il est pas fonctionnelle; réfléchissez et corrigez le avant de l'ajouter a la fonction.}

\begin{python}
for tokens in text_as_int:
last_idx = None
for idx in tokens:
if last_idx is None:
# it's the first word in a sentence
pi[idx] += 1
else:
# the last word exists, so count a transition
A[last_idx, idx] += 1

# update last idx
last_idx = idx
\end{python}

\textbf{Exercice :
Analysez le code suivant et expliquez son fonctionnement.}
\begin{python}
compute_counts([t for t, y in zip(train_text_int, Ytrain) #cont
 if y == 0], A0, pi0)
compute_counts([t for t, y in zip(train_text_int, Ytrain) #cont
 if y == 1], A1, pi1)
\end{python}
\vspace{5cm}


\subsection{Étape 3 : Normalisation}
Une fois que nous avons compté les occurrences des éléments étudier, nous allons normaliser les valeurs au moyen du calcul d'une moyenne simple.

\begin{python}
# normalize A and pi so they are valid probability matrices
# convince yourself that this is equivalent to the formulas 
#shown before
A0 /= A0.sum(axis=1, keepdims=True)
pi0 /= pi0.sum()

A1 /= A1.sum(axis=1, keepdims=True)
pi1 /= pi1.sum()
\end{python}

\subsection{Étape 4 : propriété \textit{log}}

Pour calculer le logarithme de chaque valeur de nos matrices, nous utilisons la fonction \textit{log()} de la bibliothèque \textit{numpy}.
\begin{python}
# log A and pi since we don't need the actual probs
logA0 = np.log(A0)
logpi0 = np.log(pi0)

logA1 = np.log(A1)
logpi1 = np.log(pi1)
\end{python}

\subsection{Étape 5 : \textit{priors}}

Afin de pouvoir utiliser les règles bayésiennes, nous devons aussi calculer les \textit{priors} de chaque classe. 

\begin{python}
# compute priors
count0 = sum(y == 0 for y in Ytrain)
count1 = sum(y == 1 for y in Ytrain)
total = len(Ytrain)
p0 = count0 / total
p1 = count1 / total
\end{python}
\textbf{Exercice : Décrivez la fonctionnalité de chaque ligne du code ci-dessus}
\vspace{5cm}

Et pour être consistant, ne devons aussi calculer les logarithmes.

\begin{python}
logp0 = np.log(p0)
logp1 = np.log(p1)
\end{python}

\subsection{Étape 5 : Classe \textit{classifier}}
Maintenant, nous avons tous les éléments pour réaliser une prédiction en suivant l’équation REF, nous avons :
\begin{enumerate}
	\item Créez la classe \textit{classifier} avec son constructeur, le constructeur doit recevoir les arguments : \textit{logAs, Logpis, logpriors}
	\begin{itemize}
		\item le constructeur devra initialiser des variables locales en utilisant les valeur reçues en argument.
	\end{itemize}
	\item Créez la fonction \textit{compute\_log\_likelihood()} avec deux arguments comme entrée : \textit{input\_, class\_} ET l'objet \textit{self} car on devra initialiser des variables locales.
	\begin{itemize}
		\item \textit{input\_} : Contiendra le texte à analyser déjà transformé en valeurs entières.
		\item \textit{class\_} : Contiendra l'étiquette de la classe avec la quelle le texte sera comparé (0 = Poe, 1 = Frost)
	\end{itemize}
	\item Créez la fonction \textit{predict()} avec un argument comme entrée : \textit{inputs} ET l'objet \textit{self} car on devra initialiser des variables locales.
	\begin{itemize}
		\item \textit{input} : Contiendra les textes à analyser déjà transformés en valeurs entières.
	\end{itemize}
\end{enumerate}